{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6645e3-b172-4d82-8fd4-ece4aad1fac8",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras + TensorFlow\n",
    "\n",
    "This notebook provides a very brief introduction to the theory behind deep neural networks. There are tons great resources online for digging deaper into the math behind deep learning, this introduction will be very high level and I'll gloss over some details. My goal with the conceptual intro is to give you just enough intuition to make sense of the code we'll look at later.\n",
    "\n",
    "Following the theorical foundation, we'll dive straight into some examples in code to demonstrate how to build deep neural networks in TensorFlow using Keras. We'll look at a classic image classification task using the MNIST handwritten digit dataset. From there we'll move on to time series anomoly detection with a [Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory) (LSTM) network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737cbab-72d0-445e-86d8-a45af02dfd8e",
   "metadata": {},
   "source": [
    "## Deep Learning Basics\n",
    "\n",
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a field of machine learning that uses artificial neural networks to perform learning tasks. These neural networks are called \"deep\" because they have many layers of neurons. \n",
    "\n",
    "<img src=\"images/AI-ML-DL.svg\" alt=\"deep learning in context\" width=\"400\">\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:AI-ML-DL.svg\">Original file: Avimanyu786SVG version: Tukijaaliwa</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons\n",
    "\n",
    "While the theoritical beginnings of deep learning can be traced to the work of [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in the 1960s, advances in GPU technology, [tensor cores](https://www.nvidia.com/en-us/data-center/tensor-cores/) among others, and the availability of big data have helped the field take off in recent years. GPUs are particularly important as they enable massive parallelism for the kinds of linear algebra and matrix and calculus operations used when training models.\n",
    "\n",
    "### The Artificial Neuron\n",
    "\n",
    "Essentially, a neural network is a function approximator. We'd like the network to learn a function `f(x)` that maps some input X to some prediction Y. For example, given an image of a cat, we'd like the network to learn a function that takes as inputs the pixels of an image and correctly \"predicts\" that the image as a cat.\n",
    "\n",
    "Deep neural networks are composed of layers of inter-connected neurons. Each neuron perfoms a simple linear transformation on its inputs, and passes result of that transformation through a non-linear \"activation function.\" The non-linearity is important because it allows the network to learn complex non-linear functions. Without the non-linear activation function, the network would only be able to learn linear functions.\n",
    "\n",
    "![single neuron](images/neuron.jpg)\n",
    "\n",
    "`f(x) = wx + b` \n",
    "\n",
    "(this is our old friend `y = mx + b` from back in the day except that *w and x are vectors*)\n",
    "\n",
    "`Neuron output = activation function(f(x))`\n",
    "\n",
    "AI researchers have explored different activation fuctions, and this is still an area of active research. One commmon activation function is the [Recified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) or ReLU. This simple function returns zero when its input is negative, and returns the same value for positve inputs. In Python this is just `max(0, x)`. We'll use ReLU in our experiments today.\n",
    "\n",
    "### Training Deep Neural Networks\n",
    "\n",
    "![neural network](images/network.jpg)\n",
    "\n",
    "At a high level, the process to train a neural network is as follows:\n",
    "1. Initialize the weights and biases to small, normally distributed positive numbers.\n",
    "1. Make a prediction on a small (mini) batch of the training data. Typically NNs are trained on huge datasets so it's necessary to break the training data into \"mini-batches.\" TensorFlow will do this for us automatically.\n",
    "1. Use a loss function to calculate the \"badness\" of the prediction. There are different loss functions for different learning tasks. Since the network starts with random weights and biases, the initial predictions will be pretty bad.\n",
    "1. Calculate the gradient (partial derivative) of the loss function at each of the outputs. This is a measure of how much the loss will change for a change in each output of the network.\n",
    "1. Apply [back propagation](https://en.wikipedia.org/wiki/Backpropagation) to calculate the gradient of the loss with respect to each weight in the network. Again, this is a partial derivative that tells us how a small chage in each weight will change the loss.\n",
    "1. Adjust all of the weights and biases by a small amount (\"learning rate\") in the direction of the gradient. In pseudo code this can be expressed by `weights -= learning_rate * gradients` and `biases -= learning_rate * gradients`.\n",
    "1. Repeat the process on each batch of training data, adjusting the weights each time, until the model has seen all of the batches. One pass through all of the batches of the training data is called an epoch, and typically we'll need to train for several epochs.\n",
    "\n",
    "This process of taking small steps in the direction that minimizes the loss by adjusting the model weights and biases by the learning rate times the gradient is called [gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). \n",
    "\n",
    "While it's fun to understand the theory because you can impress your friends at cocktail parties, in practice, modern deep learning frameworks like TensorFlow and PyTorch take care of nearly all of this for us.\n",
    "\n",
    "Let's jump in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372a470-f426-4b57-9205-ded67eb7b6d0",
   "metadata": {},
   "source": [
    "## The \"Hello World\" of Deep Learning\n",
    "The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) handwritten digit classification task is sort of the hello world of deep learning. The [original paper](https://en.wikipedia.org/wiki/MNIST_database) by LeCun et al is a classic worth reading.\n",
    "\n",
    "LeCun's work on MNIST in 1998 built on postal code recognition work from a decade earlier and was revolutionary at the time. Today, we can replicate the results in TensorFlow with just a few lines of Python. First, we'll train a fully connected deep neural network (DNN) on MNIST. Then we'll train a convolutional neural network (CNN) to compare the difference in performance.\n",
    "\n",
    "[Keras](https://keras.io/) is a Python API for deep learning that runs on top of TensorFlow. Check out the [Introduction to Keras for Engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/) to learn more. Keras includes many machine learning benchmark [datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), including MNIST. \n",
    "\n",
    "### Getting The MNIST Data\n",
    "\n",
    "First, we import the mnist module and use it to download training and validation datasets. In machine learning, it's important to have a separate dataset that the model has never seen to test model performance. We need to know whether the model has discovered real patterns in the data that allow it generalize to data it has never seen, or if it's simply memorized the training data ([overritting](https://en.wikipedia.org/wiki/Overfitting#Machine_learning)). By convention, our training examples are typically called X, while our labels are called Y. \n",
    "\n",
    "This [article](https://machinelearningmastery.com/difference-test-validation-datasets/) by Jason Brownleee goes into more detail on training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b4bc65-5dd5-4628-a847-691d02d6f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536350e-dd7c-4eaa-93da-c20d35f44cfd",
   "metadata": {},
   "source": [
    "The MNIST data consists of 70,000 grayscale images of handwritten digits, 60,000 training images and 10,000 validation images. The images are small, only 28x28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e568ed99-c2f1-4e47-b91b-5be2839ee147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe4e1b-1778-4947-8750-fb3cfe62d0a6",
   "metadata": {},
   "source": [
    "Similarly, we can see there are 60,000 labels for the training data and 10,000 for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d24db4-f009-4f3e-8b47-39e1ed013e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b652b23-a4b7-4d36-9720-abf7c3145c08",
   "metadata": {},
   "source": [
    "Let's look at the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a57b2e4-ea68-4b52-8f00-25b731ad6559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0db3ff8460>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3c032-f139-4bb5-baa8-f90cc90844ac",
   "metadata": {},
   "source": [
    "Our labels indicate this is a 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c62bd05-0ef7-45eb-a7b3-8104182adc61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695916f5-c90a-44e3-8956-fffe70bfa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: play with the training data. Check out the dtype, min and max values to understand what we're working with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d7845-47a5-4ee5-8f73-c4175c850a33",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Before we can train a network with this data, we need to perform some pre-processing steps. First, we'll scale the pixel values to be between 0 and 1. If you tried the `max()` and `min()` functions on the image array in the cell above, you saw that the pixel values range from 0 to 255. So we'll simply divide each pixel by 255 to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e74e08-53fd-4bbc-909b-ceb5271b2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a222ac-28a6-49dc-9353-5a3e8c5f7c7b",
   "metadata": {},
   "source": [
    "For this simple example, we're not going to deal with the images as structures. We'll just flatten each 28x28 images into a single vector of lenth 784 to demonstrate a fully connected, or dense, neural network. This isn't the best approach for dealing with images, but it works for this example. Later, we'll see a more sophisticated technique that use convolutional layers to extract features from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3c70be-ca8b-4a2e-b40f-6c27833ee1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_valid = x_valid.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0894698-41c2-4d7c-811c-4dc0ef473fa6",
   "metadata": {},
   "source": [
    "Now we need to deal with our labels. In the example above, we saw that the first label was a \"5\". Let's look at a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e938842-2d8a-4445-8433-e44b7a0cd8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14f13e-37a5-4fc7-8829-fc720f3d8b7a",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "These numerical labels won't work for training our network. Recall that in the training process, the model starts with random weights and biases, and just makes a guess. The intial guesses are pretty bad, and we use a mathematical function called a loss function to measure badness. Then we iteratively tune the weights and biases until we (hopefully) achieve acceptable performance. \n",
    "\n",
    "Consider the scenario where we show the model a 2 and it guesses 9. That's clearly incorrect, but what if the model guessed 1? That's closer. Is it a better guess? Not in this case, it's an equally bad guess. But if we use the numbers 0-9 for our training labels, and feed these into our loss function to measure badness, the model might learn that 1 is a better guess, since it's closer to the correct number. To avoid this, we'll do something called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics), or categorical encoding.\n",
    "\n",
    "Again, Keras makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6661a0c-da77-4778-9cb9-97de259c46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "n_categories = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, n_categories)\n",
    "y_valid = keras.utils.to_categorical(y_valid, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1de9d79e-3ec8-4b50-a07e-6eb7fbd7e06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edd3b563-41e9-460c-a271-f979fcab2ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558a0e0-a27f-47a0-9a7b-23b0a05b1c4f",
   "metadata": {},
   "source": [
    "After categorical encoding, we see that our first training label has changed from a `5` to an array of lenth `10`. Note that the element at index `5` is `1` and all other elements are `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420f3231-82bc-4675-9b53-f66c7703e6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f92db-f125-401b-9eb6-6af54bc6dcee",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Now we're ready to build and train our model. Keras has a nice functional API that allows you build up a model, one layer at a time. The output of one layer is the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c9d8ae4-faba-4815-be19-1531d8f4d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf1252-bb54-41be-a639-cd4c1aa5f54e",
   "metadata": {},
   "source": [
    "#### Input Layer\n",
    "\n",
    "First, we add the input layer. This layer must match the shape of our input. Since we flattened our 28x28 images into lenth 768 vectors, we need to tell Keras that the input layer has 784 inputs. We don't have to specify the number of inputs for subsequent layers as Keras will figure it out from the output of the previous layer. Here we add a dense, or fully connected, layer with 512 neurons and use the ReLU activation function.\n",
    "\n",
    "Fully connected means that every neuron in each layer is connected to every neuron in the next, like the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f25bc4b-bcc9-4b0e-8132-62e4b3c51d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=512, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06418df7-6844-4c63-99c8-81610fd81d5e",
   "metadata": {},
   "source": [
    "#### Hidden Layers\n",
    "\n",
    "Now we add the \"hidden\" layers. Hidden refers to the layers that are neither the input or the output of the network. We can have ad many of these as we want, in fact part of the work of applying deep learning to a domain problem is figuring out the most effective model architecture. How many hidden layers to use, how wide the layers should be, which activation function to use, etc. \n",
    "\n",
    "These parameters are called hyperparameters because we as modelers choose them. This is in contrast to the parameters (weights and biases) that are learned through the training process.\n",
    "\n",
    "We'll add two hidden layers, also using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85ed8ed0-acef-45dc-bf40-85eff81e1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 512, activation='relu'))\n",
    "model.add(Dense(units = 512, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720e244-8209-4365-88b6-febfd782a15f",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "\n",
    "Finally, we add the output layer. This is the layer that performs classification and gives us our predictions. The size of the output layer must match the number of classes in our data. Since MNIST contains the digits `0` through `9`, our output layer will have 10 units.\n",
    "\n",
    "In the output layer, we use a different activation function. The `softmax` function turns the model's predictions into a probability distribution across the classes. In other words, the network will output a probability for each class: 2% probability of being a `0`, 70% probability f being a `1`, 8% probability of being a `2`, etc. We choose the class with the highest predicted probability as the output of the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d1e35ed-1fd7-4e1d-b88f-7311d6df99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f0956-1bf0-4680-97f4-5f2b71ff2121",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "Keras provides a `summary()` function that we can use to print all of the layers in the model, as well as the number of trainable parameters (weights and biases) that the model has. This is referred to as the model's capacity, as more parameters generally mean the more complex patterns can be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e82ad6-3934-4ccc-a278-e3b1ed73808e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 932,362\n",
      "Trainable params: 932,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90497a39-90e6-4a8c-97b0-0725531929fa",
   "metadata": {},
   "source": [
    "#### Compile and Train\n",
    "Now we can compile and train the model. We'll train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e724edf7-e8cc-40c1-9a42-5a64b3ff1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.3769 - accuracy: 0.8833 - val_loss: 0.1190 - val_accuracy: 0.9614\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.1008 - accuracy: 0.9717 - val_loss: 0.0920 - val_accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0717 - accuracy: 0.9802 - val_loss: 0.1062 - val_accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0657 - accuracy: 0.9842 - val_loss: 0.1526 - val_accuracy: 0.9743\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0607 - accuracy: 0.9856 - val_loss: 0.1141 - val_accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, verbose=1, validation_data=(x_valid, y_valid), batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa2fd0-3e47-4850-bf68-6f2c6575241e",
   "metadata": {},
   "source": [
    "We can visualized our model using:\n",
    "\n",
    "`\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

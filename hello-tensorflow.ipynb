{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac2269-87e5-400a-98cc-079ef275cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to install Python dependencies\n",
    "!pip3 install -Uqq -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6645e3-b172-4d82-8fd4-ece4aad1fac8",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras + TensorFlow\n",
    "\n",
    "This notebook provides a very brief introduction to the theory behind deep neural networks. There are many good resources online for digging deaper into the theory and maths behind deep learning, this introduction will be very high level and I'll gloss over some details. My goal with the conceptual intro is to give you just enough intuition to make sense of the code we'll look at later.\n",
    "\n",
    "Following the theorical foundation, we'll dive straight into some examples in code to demonstrate how to build deep neural networks in TensorFlow using Keras. We'll look at a classic image classification task using the MNIST handwritten digit dataset. \n",
    "\n",
    "Objectives\n",
    "1. Gain a conceptual understanding of deep learning\n",
    "1. Understand the training process\n",
    "1. Build a \"fully connected\" neural network in Keras\n",
    "1. Understand learning curves and overfitting\n",
    "1. Learn learn now to use a trained model to make predictions.\n",
    "\n",
    "Note: if you're returning to this notebook to experiment with training, you can jump straight to the [finished MNIST model](./hello-tensorflow.ipynb#Putting-it-all-together)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737cbab-72d0-445e-86d8-a45af02dfd8e",
   "metadata": {},
   "source": [
    "## Deep Learning Basics\n",
    "\n",
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a field of machine learning that uses artificial neural networks to perform learning tasks. These neural networks are called \"deep\" because they have many layers of neurons. \n",
    "\n",
    "<img src=\"images/AI-ML-DL.svg\" alt=\"deep learning in context\" width=\"400\">\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:AI-ML-DL.svg\">Original file: Avimanyu786SVG version: Tukijaaliwa</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons\n",
    "\n",
    "The theoritical beginnings of deep learning can be traced to the work of [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in the 1960s. However, advances in GPU technology such as [tensor cores](https://www.nvidia.com/en-us/data-center/tensor-cores/) among others, and the availability of big data have helped the field take off in recent years. GPUs are particularly important as they enable massive parallelism for the kinds of linear algebra and matrix and calculus operations used when training models.\n",
    "\n",
    "### The Artificial Neuron\n",
    "\n",
    "Essentially, a neural network is a function approximator. We'd like the network to learn a function `f(x)` that maps some input X to some prediction Y. For example, given an image of a cat, we'd like the network to learn a function that takes as inputs the pixels of an image and correctly \"predicts\" that the image as a cat.\n",
    "\n",
    "Deep neural networks are composed of layers of inter-connected neurons. Each neuron perfoms a simple linear transformation on its inputs, and passes result of that transformation through a non-linear \"activation function.\" The non-linearity is important because it allows the network to learn complex non-linear functions. Without the non-linear activation function, the network would only be able to learn linear functions.\n",
    "\n",
    "![single neuron](images/neuron.jpg)\n",
    "\n",
    "`f(x) = wx + b` \n",
    "\n",
    "(this is our old friend `y = mx + b` from back in the day except that *w and x are vectors*)\n",
    "\n",
    "`Neuron output = activation_function(f(x))`\n",
    "\n",
    "AI researchers have explored different activation fuctions, and this is still an area of active research. One commmon activation function is the [Recified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) or ReLU. This simple function returns zero when its input is negative, and returns the same value for positve inputs. In Python this is just `max(0, x)`. We'll use ReLU in our experiments today.\n",
    "\n",
    "### Training Deep Neural Networks\n",
    "\n",
    "![neural network](images/network.jpg)\n",
    "\n",
    "At a high level, the process to train a neural network is as follows:\n",
    "1. Initialize the weights and biases to small, normally distributed positive numbers.\n",
    "1. Make a prediction on a small (mini) batch of the training data. Typically NNs are trained on huge datasets so it's necessary to break the training data into \"mini-batches.\" TensorFlow will do this for us automatically.\n",
    "1. Use a loss function to calculate the \"badness\" of the prediction. There are different loss functions for different learning tasks. Since the network starts with random weights and biases, the initial predictions will be pretty bad.\n",
    "1. Calculate the gradient (partial derivative) of the loss function at each of the outputs. This is a measure of how much the loss will change for a change in each output of the network.\n",
    "1. Apply [back propagation](https://en.wikipedia.org/wiki/Backpropagation) to calculate the gradient of the loss with respect to each weight in the network. Again, this is a partial derivative that tells us how a small chage in each weight will change the loss.\n",
    "1. Adjust all of the weights and biases by a small amount (\"learning rate\") in the direction of the gradient. In pseudo code this can be expressed by `weights -= learning_rate * gradients` and `biases -= learning_rate * gradients`.\n",
    "1. Repeat the process on each batch of training data, adjusting the weights each time, until the model has seen all of the batches. One pass through all of the batches of the training data is called an epoch, and typically we'll need to train for several epochs.\n",
    "\n",
    "This process of taking small steps in the direction that minimizes the loss by adjusting the model weights and biases by the learning rate times the gradient is called [gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). \n",
    "\n",
    "Let's jump into the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372a470-f426-4b57-9205-ded67eb7b6d0",
   "metadata": {},
   "source": [
    "## The \"Hello World\" of Deep Learning\n",
    "The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) handwritten digit classification task is sort of the hello world of deep learning. The [original paper](https://en.wikipedia.org/wiki/MNIST_database) by LeCun et al is a classic worth reading.\n",
    "\n",
    "LeCun's work on MNIST in 1998 built on postal code recognition work from a decade earlier and was revolutionary at the time. Today, we can replicate the results in TensorFlow with just a few lines of Python. First, we'll train a fully connected deep neural network (DNN) on MNIST.\n",
    "\n",
    "[Keras](https://keras.io/) is a Python API for deep learning that runs on top of TensorFlow. Check out the [Introduction to Keras for Engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/) to learn more. Keras includes many machine learning benchmark [datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), including MNIST. \n",
    "\n",
    "### Getting The MNIST Data\n",
    "\n",
    "First, we import the mnist module and use it to download training and validation datasets. In machine learning, it's important to have a separate dataset that the model has never seen to test model performance. We need to know whether the model has discovered real patterns in the data that allow it generalize to data it has never seen, or if it's simply memorized the training data ([overritting](https://en.wikipedia.org/wiki/Overfitting#Machine_learning)). By convention, our training examples are typically called X, while our labels are called Y. \n",
    "\n",
    "This [article](https://machinelearningmastery.com/difference-test-validation-datasets/) by Jason Brownlee goes into more detail on training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4bc65-5dd5-4628-a847-691d02d6f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536350e-dd7c-4eaa-93da-c20d35f44cfd",
   "metadata": {},
   "source": [
    "The MNIST data consists of 70,000 grayscale images of handwritten digits, 60,000 training images and 10,000 validation images. The images are small, only 28x28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568ed99-c2f1-4e47-b91b-5be2839ee147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe4e1b-1778-4947-8750-fb3cfe62d0a6",
   "metadata": {},
   "source": [
    "Similarly, we can see there are 60,000 labels for the training data and 10,000 for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d24db4-f009-4f3e-8b47-39e1ed013e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b652b23-a4b7-4d36-9720-abf7c3145c08",
   "metadata": {},
   "source": [
    "Let's look at the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57b2e4-ea68-4b52-8f00-25b731ad6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3c032-f139-4bb5-baa8-f90cc90844ac",
   "metadata": {},
   "source": [
    "Our labels indicate this is a 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62bd05-0ef7-45eb-a7b3-8104182adc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695916f5-c90a-44e3-8956-fffe70bfa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: play with the training data. Check out the dtype, min and max values to understand what we're working with.\n",
    "x_train.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d7845-47a5-4ee5-8f73-c4175c850a33",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Before we can train a network with this data, we need to perform some pre-processing steps. First, we'll scale the pixel values to be between 0 and 1. If you tried the `max()` and `min()` functions on the image array in the cell above, you saw that the pixel values range from 0 to 255. So we'll simply divide each pixel by 255 to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e74e08-53fd-4bbc-909b-ceb5271b2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a222ac-28a6-49dc-9353-5a3e8c5f7c7b",
   "metadata": {},
   "source": [
    "For this simple example, we're not going to deal with the images as structures. We'll just flatten each 28x28 images into a single vector of lenth 784 to demonstrate a fully connected, or dense, neural network. This isn't the best approach for dealing with images, but it works for this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c70be-ca8b-4a2e-b40f-6c27833ee1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_valid = x_valid.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0894698-41c2-4d7c-811c-4dc0ef473fa6",
   "metadata": {},
   "source": [
    "Now we need to deal with our labels. In the example above, we saw that the first label was a \"5\". Let's look at a few more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e938842-2d8a-4445-8433-e44b7a0cd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14f13e-37a5-4fc7-8829-fc720f3d8b7a",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "These numerical labels won't work for training our network. Recall that in the training process, the model starts with random weights and biases, and just makes a guess. The intial guesses are pretty bad, and we use a mathematical function called a loss function to measure badness. Then we iteratively tune the weights and biases until we (hopefully) achieve acceptable performance. \n",
    "\n",
    "Consider the scenario where we show the model a 2 and it guesses 9. That's clearly incorrect, but what if the model guessed 1? That's closer. Is it a better guess? Not in this case, it's an equally bad guess. But if we use the numbers 0-9 for our training labels, and feed these into our loss function to measure badness, the model might learn that 1 is a better guess, since it's closer to the correct number. To avoid this, we'll do something called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics), or categorical encoding.\n",
    "\n",
    "Again, Keras makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6661a0c-da77-4778-9cb9-97de259c46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "n_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9d79e-3ec8-4b50-a07e-6eb7fbd7e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3b563-41e9-460c-a271-f979fcab2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558a0e0-a27f-47a0-9a7b-23b0a05b1c4f",
   "metadata": {},
   "source": [
    "After categorical encoding, we see that our first training label has changed from a `5` to an array of lenth `10`. Note that the element at index `5` is `1` and all other elements are `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f3231-82bc-4675-9b53-f66c7703e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argmax(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f92db-f125-401b-9eb6-6af54bc6dcee",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Now we're ready to build and train our model. Keras has a nice functional API that allows you build up a model, one layer at a time. The output of one layer is the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d8ae4-faba-4815-be19-1531d8f4d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf1252-bb54-41be-a639-cd4c1aa5f54e",
   "metadata": {},
   "source": [
    "#### Input Layer\n",
    "\n",
    "First, we add the input layer. This layer must match the shape of our input. Since we flattened our 28x28 images into lenth 768 vectors, we need to tell Keras that the input layer has 784 inputs. We don't have to specify the number of inputs for subsequent layers as Keras will figure it out from the output of the previous layer. Here we add a dense, or fully connected, layer with 32 neurons and use the ReLU activation function.\n",
    "\n",
    "Fully connected means that every neuron in each layer is connected to every neuron in the next, like the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25bc4b-bcc9-4b0e-8132-62e4b3c51d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06418df7-6844-4c63-99c8-81610fd81d5e",
   "metadata": {},
   "source": [
    "#### Hidden Layers\n",
    "\n",
    "Now we add the \"hidden\" layers. Hidden refers to the layers that are neither the input or the output of the network. We can have ad many of these as we want, in fact part of the work of applying deep learning to a domain problem is figuring out the most effective model architecture. How many hidden layers to use, how wide the layers should be, which activation function to use, etc. \n",
    "\n",
    "These parameters are called hyperparameters because we as modelers choose them. This is in contrast to the parameters (weights and biases) that are learned through the training process.\n",
    "\n",
    "We'll add two hidden layers, also using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed8ed0-acef-45dc-bf40-85eff81e1154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720e244-8209-4365-88b6-febfd782a15f",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "\n",
    "Finally, we add the output layer. This is the layer that performs classification and gives us our predictions. The size of the output layer must match the number of classes in our data. Since MNIST contains the digits `0` through `9`, our output layer will have 10 units.\n",
    "\n",
    "In the output layer, we use a different activation function. The `softmax` function turns the model's predictions into a probability distribution across the classes. In other words, the network will output a probability for each class: 2% probability of being a `0`, 70% probability of being a `1`, 8% probability of being a `2`, etc. We choose the class with the highest predicted probability as the output of the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e35ed-1fd7-4e1d-b88f-7311d6df99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f0956-1bf0-4680-97f4-5f2b71ff2121",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "Keras provides a `summary()` function that we can use to print all of the layers in the model, as well as the number of trainable parameters (weights and biases) that the model has. This is referred to as the model's capacity, as more parameters generally mean the more complex patterns can be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e82ad6-3934-4ccc-a278-e3b1ed73808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978c4f3-d8b0-4b63-b986-afcb33f08993",
   "metadata": {},
   "source": [
    "We're starting with a very simple architecture. Note the relatively small number of trainable parameters. In contrast, GPT-3, a state of the art lanuage model has [175 billion](https://towardsdatascience.com/what-is-gpt-3-and-why-is-it-so-powerful-21ea1ba59811) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b1e19-91e1-4610-b65e-a289a5c21004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90497a39-90e6-4a8c-97b0-0725531929fa",
   "metadata": {},
   "source": [
    "#### Compile and Train\n",
    "Now we can compile and train the model. We'll train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724edf7-e8cc-40c1-9a42-5a64b3ff1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=10, verbose=1, validation_data=(x_valid, y_valid), batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ca173-7552-40b9-b7e1-ebee67171684",
   "metadata": {},
   "source": [
    "Visualizing learning and loss curves can help identify whether the model is overfitting. The function defined below graphs both and allows us to compare the performance on the training data to the performance on the validation data. If the validation loss is increasing while the training loss is decreasing, or the validation accuracy is decreasing while training accuracy is still increasing, the model it likely overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf4094-da72-4907-a622-0ff3f06ff7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_and_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Given training history, plot the learning curves\n",
    "    and the loss curves for both training and \n",
    "    validation datasets.\n",
    "    \"\"\"\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(training_accuracy))\n",
    "\n",
    "    plt.figure(constrained_layout=True, figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, training_accuracy, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac67d6-d30c-4ae2-b838-132a3cf4034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_and_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0b87a-9ae0-4e36-b737-83c1b51ee98d",
   "metadata": {},
   "source": [
    "#### Going Deeper\n",
    "Let's try adding capacity to the model. A deeper model with more parameters could perform better, but it could also overfit since it might use those parameters to memorize the training data without learning new patterns. One way to avoid overfitting is to add an [L2 regularization](https://keras.io/api/layers/regularizers/) penalty to the activations of each layer. This penalty term effectively results in smaller weight updates per training batchs. This can slow convergence, but also helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687b7d3-eaf3-4489-bcc6-666f4e3b3045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5), input_shape=(784,)), )\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5))),\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5))),\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5))),\n",
    "model.add(Dense(units = 10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff06b23-ffa3-4f4c-adbd-16ea412f3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52c553-0615-4196-9015-b596a9d66202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=10, verbose=1, validation_data=(x_valid, y_valid), batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f745d-5e87-4ca4-ac0b-6a8596a129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_and_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbedb5-26c8-40ef-befe-34dac9dc79f8",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "Here all of the code required to define and train the model. Play around with the number of layers, neurons per layer, batch size and number of training epochs to maximize performance on the validation set without overfitting. I've included all required code, including repeating import statements from elsewhere in the notebook so you can come back to this notebook in the future and jump straight to this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dec0e4-0129-4ee1-a812-a5ecd3a55e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "\n",
    "def plot_learning_and_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Given training history, plot the learning curves\n",
    "    and the loss curves for both training and \n",
    "    validation datasets.\n",
    "    \"\"\"\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(training_accuracy))\n",
    "\n",
    "    plt.figure(constrained_layout=True, figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, training_accuracy, 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "    plt.plot(epochs, validation_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# load and prep MNIST data\n",
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_valid = x_valid.reshape(10000, 784)\n",
    "\n",
    "# one-hot encode the labels\n",
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, n_classes)\n",
    "\n",
    "# define the model architecture\n",
    "# TODO: play with number of units and layers. Hint: try a few more layers and a lot more neurons...\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5), input_shape=(784,)), )\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5))),\n",
    "model.add(Dense(units=32, activation='relu', activity_regularizer=regularizers.l2(1e-5))),\n",
    "model.add(Dense(units = 10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "print(\"\\n\")\n",
    "\n",
    "# train the model\n",
    "# TODO: try training for a difference number of epochs, or with a different batch size. \n",
    "# If your GPU has enough memory, you can try a much larger batch size.\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, verbose=1, validation_data=(x_valid, y_valid), batch_size=64\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "plot_learning_and_loss_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b73fd9-aeed-4ab3-af8e-dba1430a9da5",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Now that we've trained a model, we can use it to run inference on new data. We can even export the model to load into another system for to run in production. Which raises the question, what exactly *is* the trained model? What do we have now that we've finished training? The trained model is both the architecture, and the parameters (weights and biases) learned during training. Check this [NVIDIA blog post](https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/) to learn more about inference.\n",
    "\n",
    "Use the cell below to try out your (hopefully) improved model on some new handwritten digits, courtesy of my Apple Pencil. \n",
    "\n",
    "See if your model can classify least 8/10 of these correctly. \n",
    "\n",
    "<img src=\"./images/test_digits/digit_00.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_01.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_02.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_03.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_04.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_05.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_06.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_07.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_08.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">\n",
    "<img src=\"./images/test_digits/digit_09.jpg\" alt=\"handwritten zero\" width=56 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18705ba-ac10-4e38-81be-6205db55f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./images/test_digits/\")\n",
    "correct = 0\n",
    "total = 0\n",
    "for file in sorted(path.iterdir()):\n",
    "    if (str(file)[-3:]) == \"jpg\":\n",
    "        img = image.load_img(file, target_size=(28, 28))\n",
    "\n",
    "        # covert to an array and use just the red channel (RGB image)\n",
    "        x = image.img_to_array(img)[:,:,0].astype('uint8')\n",
    "\n",
    "        # flatten the image to a lengh 784 vector as the model expects\n",
    "        x = x / x.max()\n",
    "        flat = x.reshape((1,784))\n",
    "\n",
    "        # use the model to predict the class, this returns a probability distribution\n",
    "        classes = model.predict(flat)\n",
    "        \n",
    "        # the prediction is the class with the highest probability\n",
    "        prediction = classes.argmax()\n",
    "        actual = int(str(file).split('.')[0][-1])\n",
    "        print(f\"Prediction: {prediction} Actual: {actual}\")\n",
    "\n",
    "        total += 1\n",
    "        if prediction == actual:\n",
    "            correct +=1\n",
    "\n",
    "print(f\"\\nAccuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6942f-fcf5-4791-802b-fc02bfd06974",
   "metadata": {},
   "source": [
    "## What now?\n",
    "\n",
    "Now that you understand the basics, you're ready to tackle more challenging problems! \n",
    "\n",
    "Check out the examples below for more challenging projects. Are any of these applicable to problems in your business area?\n",
    "\n",
    "1. [Timeseries Anomaly Detection](https://keras.io/examples/timeseries/timeseries_anomaly_detection/)\n",
    "1. [State of the Art Natural Language Processing](https://towardsdatascience.com/what-are-transformers-and-how-can-you-use-them-f7ccd546071a)\n",
    "1. [Sentiment Analysis](https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b500f45-6946-4056-8306-26398bd4b938",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "Here are few excellent resources if you'd like to learn more about deep learning.\n",
    "1. [Excellent neural network videos](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) from 3Blue1Brown on YouTube\n",
    "1. [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice) Coursera course\n",
    "1. [Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527) book  \n",
    "1. [Practical Deep Learning for Coders](https://course.fast.ai/) video course that goes with the book listed above\n",
    "1. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/) book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
